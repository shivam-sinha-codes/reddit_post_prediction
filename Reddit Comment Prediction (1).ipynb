{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as ts\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Originial (2000000, 15)\n",
      "NaN removed (1999977, 15)\n"
     ]
    }
   ],
   "source": [
    "df_pos_full = pd.read_csv('comments_positive.csv')\n",
    "print('Originial', df_pos_full.shape)\n",
    "df_pos_full.dropna(axis=0, inplace=True)\n",
    "print('NaN removed',df_pos_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Originial (2000000, 15)\n",
      "NaN removed (1999951, 15)\n"
     ]
    }
   ],
   "source": [
    "df_neg_full = pd.read_csv('comments_negative.csv')\n",
    "print('Originial', df_neg_full.shape)\n",
    "df_neg_full.dropna(axis=0, inplace=True)\n",
    "print('NaN removed',df_neg_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>author</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>parent_link_id</th>\n",
       "      <th>parent_text</th>\n",
       "      <th>parent_score</th>\n",
       "      <th>parent_ups</th>\n",
       "      <th>parent_author</th>\n",
       "      <th>parent_controversiality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c092j8m</td>\n",
       "      <td>t1_c092gss</td>\n",
       "      <td>t5_2qh2p</td>\n",
       "      <td>t3_8eyy3</td>\n",
       "      <td>This isn't Twitter: try to comment on the arti...</td>\n",
       "      <td>9582</td>\n",
       "      <td>9582</td>\n",
       "      <td>nraustinii</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_8eyy3</td>\n",
       "      <td>Fucking faggot.</td>\n",
       "      <td>-7526</td>\n",
       "      <td>-7526</td>\n",
       "      <td>Glorificus</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c4imcva</td>\n",
       "      <td>t1_c4im948</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_t0ynr</td>\n",
       "      <td>Well, it is exactly what it sounds like. It's ...</td>\n",
       "      <td>9531</td>\n",
       "      <td>9531</td>\n",
       "      <td>Lynfect</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_t0ynr</td>\n",
       "      <td>Elaborate on this cum box, please.</td>\n",
       "      <td>3841</td>\n",
       "      <td>3841</td>\n",
       "      <td>eeeeevil</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c0s4nfi</td>\n",
       "      <td>t1_c0s4lje</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_cf1n2</td>\n",
       "      <td>In soviet Russia, bomb disarms you!</td>\n",
       "      <td>8545</td>\n",
       "      <td>8545</td>\n",
       "      <td>CapnScumbone</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_cf1n2</td>\n",
       "      <td>I don't live in Russia anymore, and I will not...</td>\n",
       "      <td>621</td>\n",
       "      <td>621</td>\n",
       "      <td>shady8x</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c4ini33</td>\n",
       "      <td>t1_c4incln</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_t0ynr</td>\n",
       "      <td>\"runin for senitur! #YOLO!\"</td>\n",
       "      <td>7430</td>\n",
       "      <td>7430</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_t0ynr</td>\n",
       "      <td>This just made me realize that future presiden...</td>\n",
       "      <td>4651</td>\n",
       "      <td>4651</td>\n",
       "      <td>drspg99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c4imgel</td>\n",
       "      <td>t1_c4ima2e</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_t0ynr</td>\n",
       "      <td>You step motherfucker.</td>\n",
       "      <td>7173</td>\n",
       "      <td>7173</td>\n",
       "      <td>jbg89</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_t0ynr</td>\n",
       "      <td>I have sex with my step mom when my dad isn't ...</td>\n",
       "      <td>4251</td>\n",
       "      <td>4251</td>\n",
       "      <td>audir8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   parent_id subreddit_id   link_id  \\\n",
       "0  c092j8m  t1_c092gss     t5_2qh2p  t3_8eyy3   \n",
       "1  c4imcva  t1_c4im948     t5_2qh1i  t3_t0ynr   \n",
       "2  c0s4nfi  t1_c0s4lje     t5_2qh1i  t3_cf1n2   \n",
       "3  c4ini33  t1_c4incln     t5_2qh1i  t3_t0ynr   \n",
       "4  c4imgel  t1_c4ima2e     t5_2qh1i  t3_t0ynr   \n",
       "\n",
       "                                                text  score   ups  \\\n",
       "0  This isn't Twitter: try to comment on the arti...   9582  9582   \n",
       "1  Well, it is exactly what it sounds like. It's ...   9531  9531   \n",
       "2                In soviet Russia, bomb disarms you!   8545  8545   \n",
       "3                        \"runin for senitur! #YOLO!\"   7430  7430   \n",
       "4                             You step motherfucker.   7173  7173   \n",
       "\n",
       "         author  controversiality parent_link_id  \\\n",
       "0    nraustinii                 0       t3_8eyy3   \n",
       "1       Lynfect                 0       t3_t0ynr   \n",
       "2  CapnScumbone                 0       t3_cf1n2   \n",
       "3     [deleted]                 0       t3_t0ynr   \n",
       "4         jbg89                 0       t3_t0ynr   \n",
       "\n",
       "                                         parent_text  parent_score  \\\n",
       "0                                    Fucking faggot.         -7526   \n",
       "1                 Elaborate on this cum box, please.          3841   \n",
       "2  I don't live in Russia anymore, and I will not...           621   \n",
       "3  This just made me realize that future presiden...          4651   \n",
       "4  I have sex with my step mom when my dad isn't ...          4251   \n",
       "\n",
       "   parent_ups parent_author  parent_controversiality  \n",
       "0       -7526    Glorificus                        0  \n",
       "1        3841      eeeeevil                        0  \n",
       "2         621       shady8x                        0  \n",
       "3        4651       drspg99                        0  \n",
       "4        4251        audir8                        0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99999, 15)\n",
      "(99998, 15)\n"
     ]
    }
   ],
   "source": [
    "df_pos = df_pos_full.sample(frac=0.05,random_state=200)\n",
    "df_neg = df_neg_full.sample(frac=0.05,random_state=200)\n",
    "df_pos = df_pos.reset_index(drop=True)\n",
    "df_neg = df_neg.reset_index(drop=True)\n",
    "\n",
    "print(df_pos.shape)\n",
    "print(df_neg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive stats: 27.603346033460333 57.202432451660734\n",
      "Negative stats: 32.63535270705414 51.34923962338161\n"
     ]
    }
   ],
   "source": [
    "num_words_pos = df_pos['text'].apply(lambda x: len(x.split()))\n",
    "num_words_neg = df_neg['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "pos_words_mean, pos_words_std = np.mean(num_words_pos), np.std(num_words_pos)\n",
    "neg_words_mean, neg_words_std = np.mean(num_words_neg), np.std(num_words_neg)\n",
    "\n",
    "print(\"Positive stats:\", pos_words_mean, pos_words_std)\n",
    "print(\"Negative stats:\", neg_words_mean, neg_words_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text, stemming=False, stop_words=True):\n",
    "    import re\n",
    "    from string import punctuation\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk import word_tokenize\n",
    "    \n",
    "    stops = stopwords.words('english')\n",
    "    \n",
    "    # Empty comment\n",
    "    if type(text) != str or text=='':\n",
    "        return ''\n",
    "    \n",
    "    # Commence the cleaning!\n",
    "    urls = r'http(s)*:\\/\\/(\\w|\\.)+(\\/\\w+)*'\n",
    "    text = re.sub(urls, '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'re\", \" are\", text)\n",
    "    text = re.sub(\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(\"\\'d\", \" would\", text)\n",
    "    text = re.sub(\"cant\", \"can not\", text)\n",
    "    text = re.sub(\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(\"isn\\'t\", \"is not\", text)\n",
    "    text = re.sub(\"isnt\", \"is not\", text)\n",
    "    text = re.sub(\"whats\", \"what is\", text)\n",
    "    text = re.sub(\"what\\'s\", \"what is\", text)\n",
    "    text = re.sub(\"shouldn't\", \"should not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"I'm\", \"I am\", text)\n",
    "    text = re.sub(\":\", \" \", text)\n",
    "    # The comments contain \\n for line breaks, we need to remove those too\n",
    "    text = re.sub(\"\\\\n\", \" \", text)\n",
    "    \n",
    "    # Special characters\n",
    "    text = re.sub('\\&', \" and \", text)\n",
    "    text = re.sub('\\$', \" dollar \", text)\n",
    "    text = re.sub('\\%', \" percent \", text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = ''.join([word for word in text if word not in punctuation]).lower()\n",
    "    \n",
    "    # If we want to do stemming...\n",
    "    if stemming:\n",
    "        sno = SnowballStemmer('english')\n",
    "        text = ''.join([sno.stem[word] for word in text])\n",
    "    \n",
    "    # If we want to remove stop words...\n",
    "    if stop_words:\n",
    "        text = text.split()\n",
    "        text = [word for word in text if word not in stops]\n",
    "        text = ' '.join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posneg(number):\n",
    "    if number > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_pos['text'] = df_pos['text'].apply(clean).astype(str)\n",
    "df_pos['parent_text'] = df_pos['parent_text'].apply(clean).astype(str)\n",
    "df_neg['text'] = df_neg['text'].apply(clean).astype(str)\n",
    "df_neg['parent_text'] = df_neg['parent_text'].apply(clean).astype(str)\n",
    "\n",
    "df_pos['score'] = df_pos['score'].apply(posneg)\n",
    "df_pos['parent_score'] = df_pos['parent_score'].apply(posneg)\n",
    "df_neg['score'] = df_neg['score'].apply(posneg)\n",
    "df_neg['parent_score'] = df_neg['parent_score'].apply(posneg)\n",
    "\n",
    "col_write = ['text', 'score', 'ups', 'controversiality', 'parent_text', 'parent_score', 'parent_ups', 'parent_controversiality']\n",
    "df_pos_clean = df_pos[col_write].copy()\n",
    "df_neg_clean = df_neg[col_write].copy()\n",
    "df_pos_clean.dropna(axis=0, inplace=True)\n",
    "df_neg_clean.dropna(axis=0, inplace=True)\n",
    "df_pos_clean.to_csv('clean_positive_train.csv', columns=col_write, index=False)\n",
    "df_neg_clean.to_csv('clean_negative_train.csv', columns=col_write, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =open(('clean_positive_train.csv'), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = open(('clean_negative_train.csv'), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.read_csv(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg = pd.read_csv(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos['text'] = df_pos['text'].astype(str)\n",
    "df_pos['parent_text'] = df_pos['parent_text'].astype(str)\n",
    "\n",
    "df_neg['text'] = df_neg['text'].astype(str)\n",
    "df_neg['parent_text'] = df_neg['parent_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_pos, df_neg])\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (159997,)\n",
      "X_test: (40000,)\n",
      "y_train: (159997,)\n",
      "y_test: (40000,)\n"
     ]
    }
   ],
   "source": [
    "df['combined'] = df[['text', 'parent_text']].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "text_data = df['combined']\n",
    "text_score = df['score']\n",
    "# parent_text_data = df['parent_text']\n",
    "# parent_text_score = df['parent_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data,text_score, test_size = 0.20, random_state = 42)\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000, lower=True, split=' ', document_count=0)\n",
    "\n",
    "# Create the word_index list based on all our data\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "\n",
    "# Now we make a list of sequences of integers based on our texts\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(X_train_seq,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=128)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(X_test_seq,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 128\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[0]), len(train_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 4)           868984    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 868,997\n",
      "Trainable params: 868,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 4))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "#model.add(keras.layers.Dense(4, activation=tf.nn.relu))\n",
    "#model.add(keras.layers.Dense(8, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(2, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.6880 - acc: 0.5622 - val_loss: 0.6781 - val_acc: 0.6049\n",
      "Epoch 2/40\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.6619 - acc: 0.6363 - val_loss: 0.6453 - val_acc: 0.6445\n",
      "Epoch 3/40\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.6302 - acc: 0.6722 - val_loss: 0.6214 - val_acc: 0.6733\n",
      "Epoch 4/40\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.6103 - acc: 0.6869 - val_loss: 0.6105 - val_acc: 0.6872\n",
      "Epoch 5/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5983 - acc: 0.6948 - val_loss: 0.6037 - val_acc: 0.6883\n",
      "Epoch 6/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5897 - acc: 0.7013 - val_loss: 0.6001 - val_acc: 0.6910\n",
      "Epoch 7/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5832 - acc: 0.7055 - val_loss: 0.5983 - val_acc: 0.6914\n",
      "Epoch 8/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5779 - acc: 0.7082 - val_loss: 0.5965 - val_acc: 0.6928\n",
      "Epoch 9/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5734 - acc: 0.7110 - val_loss: 0.5960 - val_acc: 0.6933\n",
      "Epoch 10/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5702 - acc: 0.7133 - val_loss: 0.5954 - val_acc: 0.6921\n",
      "Epoch 11/40\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.5668 - acc: 0.7151 - val_loss: 0.5956 - val_acc: 0.6924\n",
      "Epoch 12/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5641 - acc: 0.7168 - val_loss: 0.5955 - val_acc: 0.6926\n",
      "Epoch 13/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5617 - acc: 0.7187 - val_loss: 0.5966 - val_acc: 0.6912\n",
      "Epoch 14/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5596 - acc: 0.7193 - val_loss: 0.5962 - val_acc: 0.6915\n",
      "Epoch 15/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5576 - acc: 0.7206 - val_loss: 0.5961 - val_acc: 0.6923\n",
      "Epoch 16/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5560 - acc: 0.7215 - val_loss: 0.5962 - val_acc: 0.6908\n",
      "Epoch 17/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5542 - acc: 0.7225 - val_loss: 0.5967 - val_acc: 0.6903\n",
      "Epoch 18/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5530 - acc: 0.7230 - val_loss: 0.5969 - val_acc: 0.6902\n",
      "Epoch 19/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5518 - acc: 0.7241 - val_loss: 0.5973 - val_acc: 0.6894\n",
      "Epoch 20/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5508 - acc: 0.7244 - val_loss: 0.5977 - val_acc: 0.6889\n",
      "Epoch 21/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5497 - acc: 0.7249 - val_loss: 0.5984 - val_acc: 0.6879\n",
      "Epoch 22/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5486 - acc: 0.7252 - val_loss: 0.5989 - val_acc: 0.6881\n",
      "Epoch 23/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5480 - acc: 0.7260 - val_loss: 0.5990 - val_acc: 0.6893\n",
      "Epoch 24/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5474 - acc: 0.7256 - val_loss: 0.5995 - val_acc: 0.6882\n",
      "Epoch 25/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5465 - acc: 0.7265 - val_loss: 0.5997 - val_acc: 0.6880\n",
      "Epoch 26/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5459 - acc: 0.7269 - val_loss: 0.6007 - val_acc: 0.6871\n",
      "Epoch 27/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5453 - acc: 0.7265 - val_loss: 0.6006 - val_acc: 0.6874\n",
      "Epoch 28/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5448 - acc: 0.7268 - val_loss: 0.6011 - val_acc: 0.6873\n",
      "Epoch 29/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5442 - acc: 0.7274 - val_loss: 0.6010 - val_acc: 0.6866\n",
      "Epoch 30/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5437 - acc: 0.7271 - val_loss: 0.6012 - val_acc: 0.6873\n",
      "Epoch 31/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5431 - acc: 0.7279 - val_loss: 0.6021 - val_acc: 0.6863\n",
      "Epoch 32/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5427 - acc: 0.7279 - val_loss: 0.6020 - val_acc: 0.6865\n",
      "Epoch 33/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5422 - acc: 0.7283 - val_loss: 0.6024 - val_acc: 0.6866\n",
      "Epoch 34/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5419 - acc: 0.7286 - val_loss: 0.6030 - val_acc: 0.6854\n",
      "Epoch 35/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5415 - acc: 0.7289 - val_loss: 0.6032 - val_acc: 0.6862\n",
      "Epoch 36/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5411 - acc: 0.7287 - val_loss: 0.6036 - val_acc: 0.6856\n",
      "Epoch 37/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5408 - acc: 0.7288 - val_loss: 0.6044 - val_acc: 0.6850\n",
      "Epoch 38/40\n",
      "313/313 [==============================] - 5s 18ms/step - loss: 0.5404 - acc: 0.7289 - val_loss: 0.6067 - val_acc: 0.6833\n",
      "Epoch 39/40\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5401 - acc: 0.7290 - val_loss: 0.6046 - val_acc: 0.6851\n",
      "Epoch 40/40\n",
      "313/313 [==============================] - 5s 18ms/step - loss: 0.5397 - acc: 0.7297 - val_loss: 0.6056 - val_acc: 0.6842\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(test_data, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
